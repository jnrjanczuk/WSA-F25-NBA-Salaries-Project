# -*- coding: utf-8 -*-
"""NBADataVisualization.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tK-G0aON3pTKSpQZ4R8eXoYD651EFt8u
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup as bs
from numpy._core.fromnumeric import mean
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files


# Scrape function for tables on ESPN websites
def scrape(url, headers=None):
  if headers is None:
      headers = {
          "User-Agent": "Mozilla/5.0",
        }

  s = requests.Session()
  try:
      r = requests.get(url, headers=headers, timeout=10) # Added timeout
      r.raise_for_status() # Raise an exception for HTTP errors
  except requests.exceptions.RequestException as e:
      print(f"Error accessing {url}: {e}")
      return []

  # Convert to a BeautifulSoup object
  soup = bs(r.content, features="lxml")

  # Scrape table
  table = soup.find("table")
  if table is None:
      print(f"Warning: No table element found at {url}. Returning empty list.")
      return []

  rows = table.find_all("tr", class_=["oddrow", "evenrow"])
  l = []
  for tr in rows:
    td = tr.find_all('td')
    row = [str(tr.get_text()).strip() for tr in td]
    l.append(row)

  return l

# Clean function for names in pandas dataframe
def clean(df):
  df['NAME'] = df['NAME'].str.split(',', n=1).str[0]
  return df


# Scraping NBA player salary data (df1)
print("Scraping NBA player salary data...")
headers_salary = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://www.espn.com/nba/salaries/_/year/2025",
    "sec-ch-ua": '"Chromium";v="120", "Google Chrome";v="120", "Not=A?Brand";v="24"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
  }

all_rows_df1 = []
# Scrape first page for df1
initial_url_df1 = "https://www.espn.com/nba/salaries/_/year/2025"
r1_initial = requests.get(initial_url_df1, headers=headers_salary)
soup1_initial = bs(r1_initial.content, features="lxml")
table1_initial = soup1_initial.find("table")
if table1_initial:
    columns_df1 = table1_initial.find(class_="colhead").find_all("td")
    column_names_df1 = [c.string for c in columns_df1]
    rows1_initial = table1_initial.find_all("tr", class_=["oddrow", "evenrow"])
    for tr in rows1_initial:
        td = tr.find_all('td')
        row = [str(tr.get_text()).strip() for tr in td]
        all_rows_df1.append(row)
else:
    print(f"Warning: No table found on initial page for df1: {initial_url_df1}")

# Scrape remaining pages for df1
for i in range(12):
  page_url = f"https://www.espn.com/nba/salaries/_/year/2025/page/{i+2}"
  all_rows_df1 += scrape(page_url, headers=headers_salary)

df1 = pd.DataFrame(all_rows_df1, columns=column_names_df1)


# Scraping NBA player stats (df2)
print("Scraping NBA player stats...")
headers_stats = {
    "User-Agent": "Mozilla/5.0",
  }

all_rows_df2 = []
# Scrape first page for df2
initial_url_df2 = "http://insider.espn.com/nba/hollinger/statistics/_/year/2025"
r2_initial = requests.get(initial_url_df2, headers=headers_stats)
soup2_initial = bs(r2_initial.content, features="lxml")
table2_initial = soup2_initial.find("table")
if table2_initial:
    columns_df2 = table2_initial.find(class_="colhead").find_all("td")
    column_names_df2 = [c.string for c in columns_df2]
    rows2_initial = table2_initial.find_all("tr", class_ =["oddrow", "evenrow"])
    for tr in rows2_initial:
        td = tr.find_all('td')
        row = [str(tr.get_text()).strip() for tr in td]
        all_rows_df2.append(row)
else:
    print(f"Warning: No table found on initial page for df2: {initial_url_df2}. This often indicates access restrictions for insider.espn.com.")

# Scrape remaining pages for df2
for i in range(7):
  page_url = f"http://insider.espn.com/nba/hollinger/statistics/_/page/{i+2}/year/2025"
  all_rows_df2 += scrape(page_url, headers=headers_stats)

df2 = pd.DataFrame(all_rows_df2, columns=column_names_df2)
df2.columns = ['RK', 'NAME', 'GP', 'MPG', 'TS%', 'AST', 'TO', 'USG', 'ORR', 'DRR',
       'REBR', 'PER', 'VA', 'EWA']


# Scraping for more stats (df3)
print("Scraping additional NBA stats...")
headers_more_stats = {
    "User-Agent": "Mozilla/5.0",
  }

all_rows_df3 = []
# Scrape first page for df3
initial_url_df3 = "https://www.espn.com/nba/seasonleaders/_/league/nba/sort/avgPoints/year/2025"
r3_initial = requests.get(initial_url_df3, headers=headers_more_stats)
soup3_initial = bs(r3_initial.content, features="lxml")
table3_initial = soup3_initial.find("table")
if table3_initial:
    columns_df3 = table3_initial.find(class_="colhead").find_all("td")
    column_names_df3 = [c.string for c in columns_df3]
    rows3_initial = table3_initial.find_all("tr", class_ =["oddrow", "evenrow"])
    for tr in rows3_initial:
        td = tr.find_all('td')
        row = [str(tr.get_text()).strip() for tr in td]
        all_rows_df3.append(row)
else:
    print(f"Warning: No table found on initial page for df3: {initial_url_df3}")

# Scrape remaining pages for df3
for i in range(12):
  page_url = f"https://www.espn.com/nba/seasonleaders/_/league/nba/sort/avgPoints/page/{i+2}/year/2025"
  all_rows_df3 += scrape(page_url, headers=headers_more_stats)

df3 = pd.DataFrame(all_rows_df3, columns=column_names_df3)
df3.columns = ['RK', 'NAME', 'TEAM', 'GP', 'MPG', 'FG%', 'FT%', '3PM', 'RPG', 'APG', 'STPG',
       'BLKPG', 'TOPG', 'PTS', 'ESPN']


# Clean and merge dataframes
print("Cleaning and merging dataframes...")
# Clean dataframes
df1 = clean(df1)
df2 = clean(df2)
df3 = clean(df3)

# Merge cleaned dataframes
df_merged = pd.merge(df1, df2, on='NAME', how='inner')
df_merged = pd.merge(df_merged, df3, on=['NAME', 'GP', 'MPG'], how='inner')

# Clean merged dataframe so only include qualified players (min. 20 MPG)
df_merged['MPG'] = pd.to_numeric(df_merged['MPG'], errors='coerce')
df_merged = df_merged[df_merged['MPG'] >= 20]

print("Data scraping, cleaning, and merging complete.")
print(df_merged.head())

# create copy of dataframe and convert desired stats to numeric values
df_numeric = df_merged
df_numeric['SALARY'] = df_numeric['SALARY'].astype(str)
df_numeric['SALARY'] = df_numeric['SALARY'].str.replace('$', '', regex=False)
df_numeric['SALARY'] = df_numeric['SALARY'].str.replace(',', '', regex=False)
df_numeric['SALARY'] = df_numeric['SALARY'].astype(int)
df_numeric['PER'] = pd.to_numeric(df_numeric['PER'], errors='coerce')
df_numeric['PTS'] = pd.to_numeric(df_numeric['PTS'], errors='coerce')

# linear regression of salary and PER
df_numeric['SALARY_MILLIONS'] = df_numeric['SALARY']/1000000
x = df_numeric['PER']
y = df_numeric['SALARY_MILLIONS']
x = sm.add_constant(x)
model = sm.OLS(y, x)
results = model.fit()
print(results.summary())

# modeling linear regression
plt.figure(figsize=(12, 8))
plt.scatter(df_numeric['PER'], df_numeric['SALARY_MILLIONS'])

# generate x-values for the regression line
x_vals = np.linspace(df_numeric['PER'].min(), df_numeric['PER'].max(), 100)

# regression equation
y_vals = results.params['const'] + results.params['PER'] * x_vals

# plot regression line
plt.plot(x_vals, y_vals)

plt.xlabel("PER")
plt.ylabel("Salary (USD in Millions)")
plt.title("Linear Regression of Salary vs PER")
plt.grid(True)
plt.ylim(bottom=0)
plt.show()